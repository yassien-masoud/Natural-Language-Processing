{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requesting the tweets for the users with the fake tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module \n",
    "import tweepy \n",
    "import json\n",
    "\n",
    "# assign the values accordingly \n",
    "consumer_key = \"put your consumer_key\"\n",
    "consumer_secret = \"consumer_secret\"\n",
    "access_key = \"access_key\"\n",
    "access_secret = \"access_secret\"\n",
    "\n",
    "\n",
    "# authorization of consumer key and consumer secret \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "\n",
    "# set access to user's access key and access secret \n",
    "auth.set_access_token(access_key, access_secret) \n",
    "\n",
    "# calling the api \n",
    "api = tweepy.API(auth) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the tweets of a user_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready\n",
    "def tweets_of_user_ids(user_list_id):\n",
    "    user_tweets = {}\n",
    "    notvalid=[]\n",
    "    for id_num in user_list_id:\n",
    "        user_id = id_num\n",
    "        try:\n",
    "            user_tweets[id_num] = api.user_timeline(user_id)\n",
    "        except:\n",
    "            notvalid.append(user_id)\n",
    "        \n",
    "# generating the tweets from the tweets object of the user\n",
    "    users_dict_tweets = {}\n",
    "    for user_id, statuse in user_tweets.items():\n",
    "        users_dict_tweets[user_id] = []\n",
    "        for tweet in statuse:\n",
    "           users_dict_tweets[user_id].append(tweet.text) \n",
    "    print(notvalid)\n",
    "    return users_dict_tweets\n",
    "\n",
    "tweets = tweets_of_user_ids(df_user_id_list)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning \n",
    "def str_clean(tweets_list):\n",
    "    tweets = []\n",
    "    for text in tweets_list:\n",
    "        text = text.lower()\n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        text = re.sub(r'rt' , '', text)\n",
    "        # tokenizing the text\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        PS = PorterStemmer()\n",
    "        # words = [w for w in words if not w in stop_words]\n",
    "        #words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        #print(words)\n",
    "        tweets.append(words)\n",
    "    return tweets\n",
    "        \n",
    "       \n",
    "    \n",
    "\n",
    "# Extracting tweets from the users dictionary\n",
    "def extract_tweet(dic):\n",
    "    clean_tweets={}\n",
    "    for user_id, tweets_list in dic.items():\n",
    "        clean_tweets[user_id]= str_clean(tweets_list)\n",
    "    return clean_tweets\n",
    "\n",
    "# Word counter\n",
    "def word_counter(dic):\n",
    "    tokenized_tweets = {}\n",
    "    for user_id, tweets_list in dic.items():\n",
    "        lis = []\n",
    "        for tweet in tweets_list:\n",
    "            lis.extend(word_tokenize(tweet))\n",
    "        word_count = FreqDist(lis)\n",
    "        tokenized_tweets[user_id] = word_count\n",
    "    return tokenized_tweets\n",
    "\n",
    "\n",
    "# Word counter without the stopwords \n",
    "def word_counter_stop_w(dic):\n",
    "    st_w = stopwords.words('english')\n",
    "    tokenized_tweets = {}\n",
    "    for user_id, tweets_list in dic.items():\n",
    "        lis = []\n",
    "        for tweet in tweets_list:\n",
    "            lis.extend(word_tokenize(tweet))\n",
    "        # remove stopwords\n",
    "        lis_stop_w = [word for word in lis if word not in st_w]\n",
    "        word_count = FreqDist(lis_stop_w)\n",
    "        tokenized_tweets[user_id] = word_count\n",
    "    return tokenized_tweets\n",
    "\n",
    "# Post_tag the tweets\n",
    "def pos_tag_tweets(dic):\n",
    "    pos_tag_dic = {}\n",
    "    chunk_grammar = \"NP: {<JJ><NN>}\"\n",
    "    chunk_parser = RegexpParser(chunk_grammar) \n",
    "    for user_id, tweets in dic.items():\n",
    "        tweet_list = []\n",
    "        for tweet in tweets:\n",
    "            tweet_tokenized = word_tokenize(tweet)\n",
    "            #print(tweet_tokenized)\n",
    "            tweet_pos_tag = pos_tag(tweet_tokenized)\n",
    "            #print(tweet_pos_tag)\n",
    "            [('Now', 'RB'), ('im', 'VBZ'), ('up', 'RP'), ('watch', 'JJ'), ('60', 'CD'), ('days', 'NNS'), ('in', 'IN')]\n",
    "            tweet_parser = [tag[0] for tag in tweet_pos_tag if tag[1]=='JJ']\n",
    "            #print(tweet_parser)\n",
    "            #tweet_list.append(chunk_parser.parse(tweet_parser))\n",
    "            tweet_list.append(tweet_parser)\n",
    "        pos_tag_dic[user_id] = tweet_list\n",
    "    return pos_tag_dic\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert tweets to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extr_tweets = extract_tweet(tweets) \n",
    "\n",
    "def tweets_to_string(dicto):\n",
    "    dic={}\n",
    "    for user_id, string_list in dicto.items():\n",
    "        dic[user_id]= ' '.join(string_list)\n",
    "    return dic\n",
    "extr_tweets_strings = tweets_to_string(extr_tweets)\n",
    "extr_tweets_strings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
